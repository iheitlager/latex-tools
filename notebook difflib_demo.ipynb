{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding difflib.SequenceMatcher\n",
    "\n",
    "This notebook demonstrates how `difflib.SequenceMatcher` works using the abstracts from old_file.tex and new_file.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the abstracts from both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old abstract length: 2336 characters\n",
      "New abstract length: 2354 characters\n"
     ]
    }
   ],
   "source": [
    "old_abstract = r\"\"\"\\def\\mytitle{Navigating Digital Innovation in Asset-Intensive Industries: A Process Model Informed by Design Science}\n",
    "\\def\\myabstract{Companies in asset-intensive industries, such as aviation and railways, face unique digital transformation challenges due to the misalignment between the rapid evolution of digital technologies and decades-long asset lifecycles. Existing innovation frameworks are inadequate for managing this complexity, which in turn creates tensions between innovation requirements and operational reliability demands. This paper therefore investigates how asset-intensive companies can systematically integrate digital innovations, while fully complying with regulatory constraints and safety requirements. We employ a design science approach in a study of Nederlandse Spoorwegen (NS), the Dutch national railway operator, focusing specifically on the implementation of AI-driven CCTV systems within the operations of NS. Drawing on a literature review and participant-observer as well as interview data, we develop six design propositions that address the key digital innovation challenges of asset-intensive companies in the area of market readiness assessment, modular architecture, regulatory compliance, temporal coordination, ecosystem governance, and organizational capability development. Using these design propositions, we develop the Iterative Development \\& Adoption Model (IDAM) that operationalizes market maturity assessment through market readiness levels to guide make-or-buy transitions across four iterative phases: ideate, assess, realise, and review. This model includes a Development Reference Architecture for emerging technologies and an Integration Reference Architecture for more mature technologies, enabling concurrent sourcing strategies based on technological maturity. IDAM provides actionable guidance for decisions about technology adoption in asset-intensive contexts, thereby offering a systematic approach to innovation management in industries with very long asset lifecycles and huge regulatory constraints.}\n",
    "\\def\\mykeywords{design science; engineering design; technology adoption; digital transformation; product development; asset lifecycle; modular architecture; market readiness; operational reliability}\n",
    "\n",
    "% asdasdasdasdasdadasdlkas\n",
    "\n",
    "\n",
    "1234\n",
    "what is a girl to do\n",
    "hello world\n",
    "\"\"\"\n",
    "\n",
    "new_abstract = r\"\"\"\\def\\mytitle{Navigating Digital Transformation in Asset-Intensive Companies: A Process Model Informed by Design Science}\n",
    "\\def\\myabstract{Companies in \\textbf{asset-intensive industries}, such as aviation and railways, face unique digital transformation challenges due to the misalignment between the rapid evolution of digital technologies and decades-long asset lifecycles. Existing innovation frameworks are inadequate for managing this complexity, which in turn creates tensions between innovation requirements and operational reliability demands. This paper therefore investigates how asset-intensive companies can systematically integrate digital technologies, while fully complying with regulatory constraints and safety requirements. We employ a design science approach in a study of Nederlandse Spoorwegen (NS), the Dutch national railway operator, focusing specifically on the implementation of AI-driven CCTV systems within the operations of NS. Drawing on a literature review and participant-observer as well as interview data, we develop six design propositions that address the key digital transformation challenges of asset-intensive companies in the area of market readiness assessment, modular architecture, regulatory compliance, temporal coordination, ecosystem governance, and organizational capability development. Using these design propositions, we develop the Iterative Development \\& Adoption Model (IDAM) that operationalizes market maturity assessment through market readiness levels to guide make-or-buy transitions across four iterative phases: ideate, assess, realise, and review. This model includes a Development Reference Architecture for emerging technologies and an Integration Reference Architecture for more mature technologies, enabling concurrent sourcing strategies based on technological maturity. IDAM provides actionable guidance for decisions about technology adoption in asset-intensive contexts, thereby offering a systematic approach to innovation management in industries with very long asset lifecycles and huge regulatory constraints.}\n",
    "\\def\\mykeywords{design science; engineering design; technology adoption; digital transformation; product development; asset lifecycle; modular architecture; market readiness; operational reliability}\n",
    "\n",
    "% asdasdasdasdasdadasdlkas\n",
    "\n",
    "\n",
    "1234\n",
    "\n",
    "hello world\n",
    "what is a girl to do\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Old abstract length: {len(old_abstract)} characters\")\n",
    "print(f\"New abstract length: {len(new_abstract)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a SequenceMatcher object\n",
    "\n",
    "`SequenceMatcher` compares two sequences (strings, lists, etc.) and finds matching blocks.\n",
    "\n",
    "**Key parameters:**\n",
    "- `isjunk`: A function to filter out 'junk' elements (we use `None` to keep everything)\n",
    "- `a`: The first sequence (old text)\n",
    "- `b`: The second sequence (new text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceMatcher created successfully!\n",
      "Type: <class 'difflib.SequenceMatcher'>\n"
     ]
    }
   ],
   "source": [
    "# Create a SequenceMatcher to compare the two abstracts\n",
    "matcher = difflib.SequenceMatcher(None, old_abstract, new_abstract)\n",
    "\n",
    "print(\"SequenceMatcher created successfully!\")\n",
    "print(f\"Type: {type(matcher)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get the similarity ratio\n",
    "\n",
    "The `ratio()` method returns a measure of similarity as a float in [0, 1].\n",
    "- 1.0 means the sequences are identical\n",
    "- 0.0 means they have nothing in common\n",
    "\n",
    "**Formula:** `2.0 * M / T`\n",
    "- M = number of matching characters\n",
    "- T = total number of characters in both sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity ratio: 0.9844\n",
      "Similarity percentage: 98.44%\n"
     ]
    }
   ],
   "source": [
    "ratio = matcher.ratio()\n",
    "print(f\"Similarity ratio: {ratio:.4f}\")\n",
    "print(f\"Similarity percentage: {ratio * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get matching blocks\n",
    "\n",
    "`get_matching_blocks()` returns a list of triples `(i, j, n)` where:\n",
    "- `i`: start index in sequence a (old text)\n",
    "- `j`: start index in sequence b (new text)  \n",
    "- `n`: length of the matching block\n",
    "\n",
    "This means: `a[i:i+n] == b[j:j+n]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching blocks: 3\n",
      "\n",
      "First 10 matching blocks:\n",
      "Block 0: pos_old=0, pos_new=0, length=7\n",
      "  Text: '\\def\\my'\n",
      "\n",
      "Block 1: pos_old=12, pos_new=8, length=8\n",
      "  Text: '{Navigat'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matching_blocks = matcher.get_matching_blocks()\n",
    "\n",
    "print(f\"Number of matching blocks: {len(matching_blocks)}\\n\")\n",
    "print(\"First 10 matching blocks:\")\n",
    "for i, (a_idx, b_idx, size) in enumerate(matching_blocks[:10]):\n",
    "    if size > 0:  # Skip the final dummy block\n",
    "        matching_text = old_abstract[a_idx:a_idx+size]\n",
    "        print(f\"Block {i}: pos_old={a_idx}, pos_new={b_idx}, length={size}\")\n",
    "        print(f\"  Text: '{matching_text[:50]}...'\" if len(matching_text) > 50 else f\"  Text: '{matching_text}'\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get opcodes (operations)\n",
    "\n",
    "`get_opcodes()` returns a list of 5-tuples describing how to turn sequence a into sequence b.\n",
    "\n",
    "Each tuple is: `(tag, i1, i2, j1, j2)` where:\n",
    "- `tag`: operation type ('equal', 'replace', 'delete', 'insert')\n",
    "- `i1:i2`: slice of sequence a\n",
    "- `j1:j2`: slice of sequence b\n",
    "\n",
    "**Operations:**\n",
    "- `'equal'`: `a[i1:i2] == b[j1:j2]` (no change)\n",
    "- `'replace'`: `a[i1:i2]` should be replaced by `b[j1:j2]`\n",
    "- `'delete'`: `a[i1:i2]` should be deleted (j1 == j2)\n",
    "- `'insert'`: `b[j1:j2]` should be inserted at a[i1:i1] (i1 == i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of operations: 3\n",
      "\n",
      "All operations:\n",
      "equal    a[   0:   7] == b[   0:   7] (length: 7)\n",
      "         '\\def\\my'\n",
      "\n",
      "replace  a[   7:  12] -> b[   7:   8]\n",
      "         OLD: 'title'\n",
      "         NEW: 't'\n",
      "\n",
      "equal    a[  12:  20] == b[   8:  16] (length: 8)\n",
      "         '{Navigat'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opcodes = matcher.get_opcodes()\n",
    "\n",
    "print(f\"Number of operations: {len(opcodes)}\\n\")\n",
    "print(\"All operations:\")\n",
    "for tag, i1, i2, j1, j2 in opcodes:\n",
    "    old_text = old_abstract[i1:i2]\n",
    "    new_text = new_abstract[j1:j2]\n",
    "    \n",
    "    if tag == 'equal':\n",
    "        print(f\"{tag:8s} a[{i1:4d}:{i2:4d}] == b[{j1:4d}:{j2:4d}] (length: {i2-i1})\")\n",
    "        if i2 - i1 < 100:\n",
    "            print(f\"         '{old_text}'\")\n",
    "    elif tag == 'replace':\n",
    "        print(f\"{tag:8s} a[{i1:4d}:{i2:4d}] -> b[{j1:4d}:{j2:4d}]\")\n",
    "        print(f\"         OLD: '{old_text}'\")\n",
    "        print(f\"         NEW: '{new_text}'\")\n",
    "    elif tag == 'delete':\n",
    "        print(f\"{tag:8s} a[{i1:4d}:{i2:4d}]\")\n",
    "        print(f\"         DEL: '{old_text}'\")\n",
    "    elif tag == 'insert':\n",
    "        print(f\"{tag:8s} b[{j1:4d}:{j2:4d}]\")\n",
    "        print(f\"         INS: '{new_text}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All operations with LaTeX markup:\n",
      "================================================================================\n",
      "\\def\\my\\old{title}\\new{t}{Navigat\n"
     ]
    }
   ],
   "source": [
    "print(\"All operations with LaTeX markup:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for tag, i1, i2, j1, j2 in opcodes:\n",
    "    old_text = old_abstract[i1:i2]\n",
    "    new_text = new_abstract[j1:j2]\n",
    "    \n",
    "    if tag == 'equal':\n",
    "        print(old_text, end='')\n",
    "    elif tag == 'replace':\n",
    "        print(f\"\\\\old{{{old_text}}}\\\\new{{{new_text}}}\", end='')\n",
    "    elif tag == 'delete':\n",
    "        print(f\"\\\\old{{{old_text}}}\", end='')\n",
    "    elif tag == 'insert':\n",
    "        print(f\"\\\\new{{{new_text}}}\", end='')\n",
    "\n",
    "print()  # Final newline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tokenizer:\n",
      "================================================================================\n",
      "\n",
      "Input:  in \\textbf{asset-intensive industries}, such\n",
      "Tokens: ['in', ' ', '\\\\textbf', '{', 'asset', '-', 'intensive', ' ', 'industries', '}', ',', ' ', 'such']\n",
      "Rejoined: in \\textbf{asset-intensive industries}, such\n",
      "Match: True\n",
      "\n",
      "Input:  \\def\\mytitle{Some Title}\n",
      "\n",
      "Tokens: ['\\\\def', '\\\\mytitle', '{', 'Some', ' ', 'Title', '}', '\\n']\n",
      "Rejoined: \\def\\mytitle{Some Title}\n",
      "\n",
      "Match: True\n",
      "\n",
      "Input:  text with \\& special chars\n",
      "Tokens: ['text', ' ', 'with', ' ', '\\\\&', ' ', 'special', ' ', 'chars']\n",
      "Rejoined: text with \\& special chars\n",
      "Match: True\n",
      "\n",
      "Input:  math: $\\alpha + \\beta$\n",
      "Tokens: ['math', ':', ' ', '$', '\\\\alpha', ' ', '+', ' ', '\\\\beta', '$']\n",
      "Rejoined: math: $\\alpha + \\beta$\n",
      "Match: True\n",
      "\n",
      "Input:  % This is a comment line\n",
      "Next line.\n",
      "Tokens: ['% This is a comment line', '\\n', 'Next', ' ', 'line', '.']\n",
      "Rejoined: % This is a comment line\n",
      "Next line.\n",
      "Match: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "<>:5: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "/var/folders/qg/pw0plgj16_l6pr49lgf0lghc0000gn/T/ipykernel_2594/300664428.py:5: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "  - LaTeX commands with all arguments: \\textbf{...}, \\def\\cmd{...}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize_latex(text):\n",
    "    \"\"\"Split LaTeX into meaningful tokens.\n",
    "    \n",
    "    Token types:\n",
    "    - LaTeX commands with all arguments: \\textbf{...}, \\def\\cmd{...}\n",
    "    - Single braces/brackets: { } [ ]\n",
    "    - Words (alphanumeric sequences)\n",
    "    - Whitespace (preserved)\n",
    "    - Punctuation and special characters\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Order matters! Try to match longer patterns first\n",
    "    pattern = r'''\n",
    "        \\\\[a-zA-Z]+\\*?                    # LaTeX command (e.g., \\textbf, \\section*)\n",
    "        |\\\\[^a-zA-Z]                       # Single-char commands (e.g., \\\\, \\&, \\{)\n",
    "        |[\\{\\}\\[\\]]                        # Braces and brackets (separate tokens)\n",
    "        |\\w+                               # Words (letters, digits, underscore)\n",
    "        |[ \\t]+                            # Horizontal whitespace (keep together)\n",
    "        |\\n                                # Newlines (separate token)\n",
    "        |%[^\\n]*                           # Comments (% to end of line)\n",
    "        |[^\\w\\s\\\\{}\\[\\]%]+                 # Punctuation/special chars\n",
    "        '''\n",
    "    return re.findall(pattern, text, re.VERBOSE)\n",
    "\n",
    "# Test the tokenizer\n",
    "test_cases = [\n",
    "    r\"in \\textbf{asset-intensive industries}, such\",\n",
    "    r\"\\def\\mytitle{Some Title}\" + \"\\n\",\n",
    "    r\"text with \\& special chars\",\n",
    "    r\"math: $\\alpha + \\beta$\",\n",
    "    r\"% This is a comment line\" + \"\\n\" + \"Next line.\",\n",
    "]\n",
    "\n",
    "print(\"Testing tokenizer:\")\n",
    "print(\"=\" * 80)\n",
    "for test in test_cases:\n",
    "    tokens = tokenize_latex(test)\n",
    "    print(f\"\\nInput:  {test}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Rejoined: {''.join(tokens)}\")\n",
    "    print(f\"Match: {test == ''.join(tokens)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Comparing abstracts with improved tokenizer:\n",
      "================================================================================\n",
      "\\def\\mytitle{Navigating Digital \\old{Innovation}\\new{Transformation} in Asset-Intensive \\old{Industries}\\new{Companies}: A Process Model Informed by Design Science}\n",
      "\\def\\myabstract{Companies in \\new{\\textbf{}asset-intensive industries\\new{}}, such as aviation and railways, face unique digital transformation challenges due to the misalignment between the rapid evolution of digital technologies and decades-long asset lifecycles. Existing innovation frameworks are inadequate for managing this complexity, which in turn creates tensions between innovation requirements and operational reliability demands. This paper therefore investigates how asset-intensive companies can systematically integrate digital \\old{innovations}\\new{technologies}, while fully complying with regulatory constraints and safety requirements. We employ a design science approach in a study of Nederlandse Spoorwegen (NS), the Dutch national railway operator, focusing specifically on the implementation of AI-driven CCTV systems within the operations of NS. Drawing on a literature review and participant-observer as well as interview data, we develop six design propositions that address the key digital \\old{innovation}\\new{transformation} challenges of asset-intensive companies in the area of market readiness assessment, modular architecture, regulatory compliance, temporal coordination, ecosystem governance, and organizational capability development. Using these design propositions, we develop the Iterative Development \\& Adoption Model (IDAM) that operationalizes market maturity assessment through market readiness levels to guide make-or-buy transitions across four iterative phases: ideate, assess, realise, and review. This model includes a Development Reference Architecture for emerging technologies and an Integration Reference Architecture for more mature technologies, enabling concurrent sourcing strategies based on technological maturity. IDAM provides actionable guidance for decisions about technology adoption in asset-intensive contexts, thereby offering a systematic approach to innovation management in industries with very long asset lifecycles and huge regulatory constraints.}\n",
      "\\def\\mykeywords{design science; engineering design; technology adoption; digital transformation; product development; asset lifecycle; modular architecture; market readiness; operational reliability}\n",
      "\n",
      "% asdasdasdasdasdadasdlkas\n",
      "\n",
      "\n",
      "1234\n",
      "\\new{\n",
      "hello world\n",
      "}what is a girl to do\n",
      "\\old{hello world\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Comparing abstracts with improved tokenizer:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "old_tokens = tokenize_latex(old_abstract)\n",
    "new_tokens = tokenize_latex(new_abstract)\n",
    "matcher = difflib.SequenceMatcher(None, old_tokens, new_tokens)\n",
    "opcodes = matcher.get_opcodes()\n",
    "\n",
    "for tag, i1, i2, j1, j2 in opcodes:\n",
    "    old_text = ''.join(old_tokens[i1:i2])\n",
    "    new_text = ''.join(new_tokens[j1:j2])\n",
    "\n",
    "    if tag == 'equal':\n",
    "        print(old_text, end='')\n",
    "    elif tag == 'replace':\n",
    "        print(f\"\\\\old{{{old_text}}}\\\\new{{{new_text}}}\", end='')\n",
    "    elif tag == 'delete':\n",
    "        print(f\"\\\\old{{{old_text}}}\", end='')\n",
    "    elif tag == 'insert':\n",
    "        print(f\"\\\\new{{{new_text}}}\", end='')\n",
    "\n",
    "print()  # Final newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', ' ', 'is', ' ', '\\\\textbf', '{', 'bold', '}', ' ', 'text', ' ', 'with', ' ', '\\\\alpha', '.', '\\n', 'Companies', ' ', 'in', ' ', '\\\\textbf', '{', 'asset', '-', 'intensive', '}', ' ', 'industries']\n",
      "This is \\old{\\textbf}\\new{\\textit}{\\old{bold}\\new{italic}} text with \\old{\\alpha}\\new{\\beta}.\n",
      "Companies in \\old{\\textbf{}asset-intensive\\old{}} industries\n"
     ]
    }
   ],
   "source": [
    "text1 = r\"This is \\textbf{bold} text with \\alpha.\" + \"\\n\" + r\"Companies in \\textbf{asset-intensive} industries\"\n",
    "text2 = r\"This is \\textit{italic} text with \\beta.\" + \"\\n\" + r\"Companies in asset-intensive industries\"\n",
    "\n",
    "tokens1 = tokenize_latex(text1)\n",
    "tokens2 = tokenize_latex(text2)\n",
    "\n",
    "print(tokens1)\n",
    "matcher = difflib.SequenceMatcher(None, tokens1, tokens2)\n",
    "opcodes = matcher.get_opcodes()\n",
    "end_tokens = []\n",
    "for tag, i1, i2, j1, j2 in opcodes:\n",
    "    old_tokens = tokens1[i1:i2]\n",
    "    new_tokens = tokens2[j1:j2]\n",
    "\n",
    "    if tag == 'equal':\n",
    "        end_tokens += old_tokens\n",
    "    elif tag == 'replace':\n",
    "        end_tokens += ['\\\\old', '{'] + old_tokens + ['}', '\\\\new', '{'] + new_tokens + ['}'] \n",
    "    elif tag == 'delete':\n",
    "        end_tokens += ['\\\\old', '{'] + old_tokens + ['}']\n",
    "    elif tag == 'insert':\n",
    "        end_tokens += ['\\\\new', '{'] + new_tokens + ['}']\n",
    "\n",
    "print(\"\".join(end_tokens))  # Final newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using merged opcodes (min_equal_length=10):\n",
      "================================================================================\n",
      "\\def\\old{\\mytitle{Navigating Di}\\new{\\mytitle{Navigating}gi\n"
     ]
    }
   ],
   "source": [
    "def merge_opcodes(opcodes, old_text, new_text, min_equal_length=10):\n",
    "    \"\"\"\n",
    "    Merge opcodes to avoid fragmented changes.\n",
    "    If 'equal' segments between changes are very short, treat them as part of the change.\n",
    "    \"\"\"\n",
    "    if not opcodes:\n",
    "        return opcodes\n",
    "    \n",
    "    merged = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(opcodes):\n",
    "        tag, i1, i2, j1, j2 = opcodes[i]\n",
    "        \n",
    "        # Start accumulating if it's a change operation\n",
    "        if tag in ('replace', 'delete', 'insert'):\n",
    "            # Look ahead to see if we should merge with next operations\n",
    "            while i + 1 < len(opcodes):\n",
    "                next_tag, next_i1, next_i2, next_j1, next_j2 = opcodes[i + 1]\n",
    "                \n",
    "                # If next is 'equal' and short, check if there's another change after\n",
    "                if next_tag == 'equal' and (next_i2 - next_i1) < min_equal_length:\n",
    "                    # Look at the operation after the short equal segment\n",
    "                    if i + 2 < len(opcodes):\n",
    "                        after_tag, _, _, _, _ = opcodes[i + 2]\n",
    "                        if after_tag in ('replace', 'delete', 'insert'):\n",
    "                            # Merge: extend current operation through the equal segment\n",
    "                            i2 = next_i2\n",
    "                            j2 = next_j2\n",
    "                            i += 1  # Skip the short equal\n",
    "                            \n",
    "                            # Now merge the following change operation too\n",
    "                            i += 1\n",
    "                            tag2, i1_2, i2_2, j1_2, j2_2 = opcodes[i]\n",
    "                            i2 = i2_2\n",
    "                            j2 = j2_2\n",
    "                            tag = 'replace'  # Combined operation becomes 'replace'\n",
    "                            continue\n",
    "                \n",
    "                break\n",
    "            \n",
    "            merged.append((tag, i1, i2, j1, j2))\n",
    "        else:\n",
    "            # Keep 'equal' as-is\n",
    "            merged.append((tag, i1, i2, j1, j2))\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return merged\n",
    "\n",
    "print(\"Using merged opcodes (min_equal_length=10):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "merged_opcodes = merge_opcodes(opcodes, old_abstract, new_abstract, min_equal_length=10)\n",
    "\n",
    "for tag, i1, i2, j1, j2 in merged_opcodes:\n",
    "    old_text = old_abstract[i1:i2]\n",
    "    new_text = new_abstract[j1:j2]\n",
    "    \n",
    "    if tag == 'equal':\n",
    "        print(old_text, end='')\n",
    "    elif tag == 'replace':\n",
    "        print(f\"\\\\old{{{old_text}}}\\\\new{{{new_text}}}\", end='')\n",
    "    elif tag == 'delete':\n",
    "        print(f\"\\\\old{{{old_text}}}\", end='')\n",
    "    elif tag == 'insert':\n",
    "        print(f\"\\\\new{{{new_text}}}\", end='')\n",
    "\n",
    "print()  # Final newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative approach: Token-level comparison\n",
    "\n",
    "Instead of character-by-character comparison, we can tokenize the text (words + LaTeX commands) and compare at that level. This naturally groups LaTeX markup with the content it wraps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old abstract: 639 tokens\n",
      "New abstract: 642 tokens\n",
      "\n",
      "First 30 tokens of new abstract:\n",
      "['\\\\def', '\\\\mytitle', '{', 'Navigating', ' ', 'Digital', ' ', 'Transformation', ' ', 'in', ' ', 'Asset', '-', 'Intensive', ' ', 'Companies', ':', ' ', 'A', ' ', 'Process', ' ', 'Model', ' ', 'Informed', ' ', 'by', ' ', 'Design', ' ']\n",
      "\n",
      "Token-level similarity: 98.52%\n",
      "\n",
      "Token-level differences:\n",
      "\n",
      "REPLACE:\n",
      "  Old: 'Innovation'\n",
      "  New: 'Transformation'\n",
      "\n",
      "REPLACE:\n",
      "  Old: 'Industries'\n",
      "  New: 'Companies'\n",
      "\n",
      "INSERT:\n",
      "  Old: ''\n",
      "  New: '\\textbf{'\n",
      "\n",
      "INSERT:\n",
      "  Old: ''\n",
      "  New: '}'\n",
      "\n",
      "REPLACE:\n",
      "  Old: 'innovations'\n",
      "  New: 'technologies'\n",
      "\n",
      "REPLACE:\n",
      "  Old: 'innovation'\n",
      "  New: 'transformation'\n",
      "\n",
      "INSERT:\n",
      "  Old: ''\n",
      "  New: '\n",
      "\n",
      "hello world'\n",
      "\n",
      "DELETE:\n",
      "  Old: 'hello world\n",
      "'\n",
      "  New: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "<>:6: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "/var/folders/qg/pw0plgj16_l6pr49lgf0lghc0000gn/T/ipykernel_2594/1492337232.py:6: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "  - LaTeX commands (\\textbf, \\def, etc.)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_latex(text):\n",
    "    \"\"\"\n",
    "    Tokenize LaTeX text into meaningful chunks:\n",
    "    - LaTeX commands (\\textbf, \\def, etc.)\n",
    "    - Braces and special characters\n",
    "    - Words and whitespace\n",
    "    \"\"\"\n",
    "    # Pattern: LaTeX commands | special chars | words | whitespace\n",
    "    pattern = r'(\\\\[a-zA-Z]+|[{}]|\\w+|\\s+|[^\\w\\s\\\\{}]+)'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# Tokenize both abstracts\n",
    "old_tokens = tokenize_latex(old_abstract)\n",
    "new_tokens = tokenize_latex(new_abstract)\n",
    "\n",
    "print(f\"Old abstract: {len(old_tokens)} tokens\")\n",
    "print(f\"New abstract: {len(new_tokens)} tokens\")\n",
    "print(f\"\\nFirst 30 tokens of new abstract:\")\n",
    "print(new_tokens[:30])\n",
    "\n",
    "# Compare at token level\n",
    "token_matcher = difflib.SequenceMatcher(None, old_tokens, new_tokens)\n",
    "\n",
    "print(f\"\\nToken-level similarity: {token_matcher.ratio():.2%}\")\n",
    "print(\"\\nToken-level differences:\")\n",
    "\n",
    "for tag, i1, i2, j1, j2 in token_matcher.get_opcodes():\n",
    "    if tag != 'equal':\n",
    "        old_segment = ''.join(old_tokens[i1:i2])\n",
    "        new_segment = ''.join(new_tokens[j1:j2])\n",
    "        \n",
    "        print(f\"\\n{tag.upper()}:\")\n",
    "        print(f\"  Old: '{old_segment}'\")\n",
    "        print(f\"  New: '{new_segment}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-level comparison with LaTeX markup:\n",
      "================================================================================\n",
      "\\def\\mytitle{Navigating Digital \\old{Innovation}\\new{Transformation} in Asset-Intensive \\old{Industries}\\new{Companies}: A Process Model Informed by Design Science}\n",
      "\\def\\myabstract{Companies in \\new{\\textbf{}asset-intensive industries\\new{}}, such as aviation and railways, face unique digital transformation challenges due to the misalignment between the rapid evolution of digital technologies and decades-long asset lifecycles. Existing innovation frameworks are inadequate for managing this complexity, which in turn creates tensions between innovation requirements and operational reliability demands. This paper therefore investigates how asset-intensive companies can systematically integrate digital \\old{innovations}\\new{technologies}, while fully complying with regulatory constraints and safety requirements. We employ a design science approach in a study of Nederlandse Spoorwegen (NS), the Dutch national railway operator, focusing specifically on the implementation of AI-driven CCTV systems within the operations of NS. Drawing on a literature review and participant-observer as well as interview data, we develop six design propositions that address the key digital \\old{innovation}\\new{transformation} challenges of asset-intensive companies in the area of market readiness assessment, modular architecture, regulatory compliance, temporal coordination, ecosystem governance, and organizational capability development. Using these design propositions, we develop the Iterative Development & Adoption Model (IDAM) that operationalizes market maturity assessment through market readiness levels to guide make-or-buy transitions across four iterative phases: ideate, assess, realise, and review. This model includes a Development Reference Architecture for emerging technologies and an Integration Reference Architecture for more mature technologies, enabling concurrent sourcing strategies based on technological maturity. IDAM provides actionable guidance for decisions about technology adoption in asset-intensive contexts, thereby offering a systematic approach to innovation management in industries with very long asset lifecycles and huge regulatory constraints.}\n",
      "\\def\\mykeywords{design science; engineering design; technology adoption; digital transformation; product development; asset lifecycle; modular architecture; market readiness; operational reliability}\n",
      "\n",
      "% asdasdasdasdasdadasdlkas\n",
      "\n",
      "\n",
      "1234\\new{\n",
      "\n",
      "hello world}\n",
      "what is a girl to do\n",
      "\\old{hello world\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Token-level comparison with LaTeX markup:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for tag, i1, i2, j1, j2 in token_matcher.get_opcodes():\n",
    "    old_segment = ''.join(old_tokens[i1:i2])\n",
    "    new_segment = ''.join(new_tokens[j1:j2])\n",
    "    \n",
    "    if tag == 'equal':\n",
    "        print(old_segment, end='')\n",
    "    elif tag == 'replace':\n",
    "        print(f\"\\\\old{{{old_segment}}}\\\\new{{{new_segment}}}\", end='')\n",
    "    elif tag == 'delete':\n",
    "        print(f\"\\\\old{{{old_segment}}}\", end='')\n",
    "    elif tag == 'insert':\n",
    "        print(f\"\\\\new{{{new_segment}}}\", end='')\n",
    "\n",
    "print()  # Final newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical example with shorter strings\n",
    "\n",
    "Let's look at a simpler example to understand the mechanics better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SIMPLE EXAMPLE\n",
      "============================================================\n",
      "Old: 'The quick brown fox jumps'\n",
      "New: 'The quick red fox leaps'\n",
      "\n",
      "Similarity: 75.00%\n",
      "\n",
      "Matching blocks:\n",
      "  'The quick ' at old[0:10] == new[0:10]\n",
      "  'r' at old[11:12] == new[10:11]\n",
      "  ' fox ' at old[15:20] == new[13:18]\n",
      "  'ps' at old[23:25] == new[21:23]\n",
      "\n",
      "Operations to transform old -> new:\n",
      "  KEEP:    'The quick '\n",
      "  DELETE:  'b'\n",
      "  KEEP:    'r'\n",
      "  REPLACE: 'own' -> 'ed'\n",
      "  KEEP:    ' fox '\n",
      "  REPLACE: 'jum' -> 'lea'\n",
      "  KEEP:    'ps'\n"
     ]
    }
   ],
   "source": [
    "# Simple example\n",
    "old_text = \"The quick brown fox jumps\"\n",
    "new_text = \"The quick red fox leaps\"\n",
    "\n",
    "simple_matcher = difflib.SequenceMatcher(None, old_text, new_text)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SIMPLE EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Old: '{old_text}'\")\n",
    "print(f\"New: '{new_text}'\")\n",
    "print(f\"\\nSimilarity: {simple_matcher.ratio():.2%}\\n\")\n",
    "\n",
    "print(\"Matching blocks:\")\n",
    "for i, j, n in simple_matcher.get_matching_blocks():\n",
    "    if n > 0:\n",
    "        print(f\"  '{old_text[i:i+n]}' at old[{i}:{i+n}] == new[{j}:{j+n}]\")\n",
    "\n",
    "print(\"\\nOperations to transform old -> new:\")\n",
    "for tag, i1, i2, j1, j2 in simple_matcher.get_opcodes():\n",
    "    if tag == 'equal':\n",
    "        print(f\"  KEEP:    '{old_text[i1:i2]}'\")\n",
    "    elif tag == 'replace':\n",
    "        print(f\"  REPLACE: '{old_text[i1:i2]}' -> '{new_text[j1:j2]}'\")\n",
    "    elif tag == 'delete':\n",
    "        print(f\"  DELETE:  '{old_text[i1:i2]}'\")\n",
    "    elif tag == 'insert':\n",
    "        print(f\"  INSERT:  '{new_text[j1:j2]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using SequenceMatcher for word-level comparison\n",
    "\n",
    "Instead of comparing character by character, we can compare word by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old abstract: 284 words\n",
      "New abstract: 284 words\n",
      "Word-level similarity: 97.18%\n",
      "\n",
      "Word-level differences:\n",
      "\n",
      "REPLACE:\n",
      "  Position: words 2-3 -> words 2-3\n",
      "  Old: 'Innovation'\n",
      "  New: 'Transformation'\n",
      "\n",
      "REPLACE:\n",
      "  Position: words 5-6 -> words 5-6\n",
      "  Old: 'Industries:'\n",
      "  New: 'Companies:'\n",
      "\n",
      "REPLACE:\n",
      "  Position: words 15-17 -> words 15-17\n",
      "  Old: 'asset-intensive industries,'\n",
      "  New: '\\textbf{asset-intensive industries},'\n",
      "\n",
      "REPLACE:\n",
      "  Position: words 74-75 -> words 74-75\n",
      "  Old: 'innovations,'\n",
      "  New: 'technologies,'\n",
      "\n",
      "REPLACE:\n",
      "  Position: words 138-139 -> words 138-139\n",
      "  Old: 'innovation'\n",
      "  New: 'transformation'\n",
      "\n",
      "INSERT:\n",
      "  Position: words 276-276 -> words 276-278\n",
      "  Inserted: 'hello world'\n",
      "\n",
      "DELETE:\n",
      "  Position: words 282-284 -> words 284-284\n",
      "  Deleted: 'hello world'\n"
     ]
    }
   ],
   "source": [
    "# Split abstracts into words\n",
    "old_words = old_abstract.split()\n",
    "new_words = new_abstract.split()\n",
    "\n",
    "word_matcher = difflib.SequenceMatcher(None, old_words, new_words)\n",
    "\n",
    "print(f\"Old abstract: {len(old_words)} words\")\n",
    "print(f\"New abstract: {len(new_words)} words\")\n",
    "print(f\"Word-level similarity: {word_matcher.ratio():.2%}\\n\")\n",
    "\n",
    "print(\"Word-level differences:\")\n",
    "for tag, i1, i2, j1, j2 in word_matcher.get_opcodes():\n",
    "    if tag != 'equal':\n",
    "        old_segment = ' '.join(old_words[i1:i2])\n",
    "        new_segment = ' '.join(new_words[j1:j2])\n",
    "        \n",
    "        print(f\"\\n{tag.upper()}:\")\n",
    "        print(f\"  Position: words {i1}-{i2} -> words {j1}-{j2}\")\n",
    "        if tag == 'replace':\n",
    "            print(f\"  Old: '{old_segment}'\")\n",
    "            print(f\"  New: '{new_segment}'\")\n",
    "        elif tag == 'delete':\n",
    "            print(f\"  Deleted: '{old_segment}'\")\n",
    "        elif tag == 'insert':\n",
    "            print(f\"  Inserted: '{new_segment}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key takeaways about SequenceMatcher\n",
    "\n",
    "1. **Initialization**: `SequenceMatcher(isjunk, a, b)` creates a matcher for two sequences\n",
    "\n",
    "2. **Similarity ratio**: `ratio()` returns a float [0, 1] indicating how similar the sequences are\n",
    "\n",
    "3. **Matching blocks**: `get_matching_blocks()` returns `(i, j, n)` tuples showing where sequences match\n",
    "   - `a[i:i+n] == b[j:j+n]`\n",
    "\n",
    "4. **Operations**: `get_opcodes()` returns `(tag, i1, i2, j1, j2)` tuples describing transformations\n",
    "   - 'equal': sequences match\n",
    "   - 'replace': substitute a[i1:i2] with b[j1:j2]\n",
    "   - 'delete': remove a[i1:i2]\n",
    "   - 'insert': add b[j1:j2]\n",
    "\n",
    "5. **Flexibility**: Works with any sequences (strings, lists, etc.) and at any granularity (chars, words, lines)\n",
    "\n",
    "6. **Use cases**: \n",
    "   - Text diff tools\n",
    "   - Finding changes between documents\n",
    "   - Plagiarism detection\n",
    "   - Version control systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  REPLACED: \\def\\mytitle{Navigating Digital \\old{Innovation}\\new{Transformation} in Asset-Intensive \\old{Industries}\\new{Companies}: A Process Model Informed by Design Science}\n",
      "  REPLACED: \\def\\myabstract{Companies in \\new{\\textbf{}asset-intensive industries\\new{}}, such as aviation and railways, face unique digital transformation challenges due to the misalignment between the rapid evolution of digital technologies and decades-long asset lifecycles. Existing innovation frameworks are inadequate for managing this complexity, which in turn creates tensions between innovation requirements and operational reliability demands. This paper therefore investigates how asset-intensive companies can systematically integrate digital \\old{innovations}\\new{technologies}, while fully complying with regulatory constraints and safety requirements. We employ a design science approach in a study of Nederlandse Spoorwegen (NS), the Dutch national railway operator, focusing specifically on the implementation of AI-driven CCTV systems within the operations of NS. Drawing on a literature review and participant-observer as well as interview data, we develop six design propositions that address the key digital \\old{innovation}\\new{transformation} challenges of asset-intensive companies in the area of market readiness assessment, modular architecture, regulatory compliance, temporal coordination, ecosystem governance, and organizational capability development. Using these design propositions, we develop the Iterative Development & Adoption Model (IDAM) that operationalizes market maturity assessment through market readiness levels to guide make-or-buy transitions across four iterative phases: ideate, assess, realise, and review. This model includes a Development Reference Architecture for emerging technologies and an Integration Reference Architecture for more mature technologies, enabling concurrent sourcing strategies based on technological maturity. IDAM provides actionable guidance for decisions about technology adoption in asset-intensive contexts, thereby offering a systematic approach to innovation management in industries with very long asset lifecycles and huge regulatory constraints.}\n",
      "\n",
      "  KEEP:    '['\\\\def\\\\mykeywords{design science; engineering design; technology adoption; digital transformation; product development; asset lifecycle; modular architecture; market readiness; operational reliability}', '', '% asdasdasdasdasdadasdlkas', '', '', '1234']'\n",
      "  INSERT:  '['', 'hello world']'\n",
      "  KEEP:    '['what is a girl to do']'\n",
      "  DELETE:  '['hello world']'\n"
     ]
    }
   ],
   "source": [
    "old_lines = old_abstract.splitlines()\n",
    "new_lines = new_abstract.splitlines()\n",
    "\n",
    "simple_matcher = difflib.SequenceMatcher(None, old_lines, new_lines)\n",
    "\n",
    "for tag, i1, i2, j1, j2 in simple_matcher.get_opcodes():\n",
    "    if tag == 'equal':\n",
    "        print(f\"  KEEP:    '{old_lines[i1:i2]}'\")\n",
    "    elif tag == 'replace':\n",
    "        # Do a finer token-level comparison for each replaced line\n",
    "        for line_idx in range(i1, i2):\n",
    "            old_line = old_lines[line_idx]\n",
    "            # Find corresponding new line (if exists)\n",
    "            new_line_idx = j1 + (line_idx - i1)\n",
    "            if new_line_idx < j2:\n",
    "                new_line = new_lines[new_line_idx]\n",
    "            \n",
    "            old_line_tokens = tokenize_latex(old_line)\n",
    "            new_line_tokens = tokenize_latex(new_line)\n",
    "            \n",
    "            line_matcher = difflib.SequenceMatcher(None, old_line_tokens, new_line_tokens)\n",
    "            \n",
    "            # print(f\"    Line {line_idx} -> {new_line_idx}: \", end='')\n",
    "            print(f\"  REPLACED: \", end=\"\")\n",
    "            for token_tag, ti1, ti2, tj1, tj2 in line_matcher.get_opcodes():\n",
    "                old_segment = ''.join(old_line_tokens[ti1:ti2])\n",
    "                new_segment = ''.join(new_line_tokens[tj1:tj2])\n",
    "                \n",
    "                if token_tag == 'equal':\n",
    "                    print(old_segment, end='')\n",
    "                elif token_tag == 'replace':\n",
    "                    print(f\"\\\\old{{{old_segment}}}\\\\new{{{new_segment}}}\", end='')\n",
    "                elif token_tag == 'delete':\n",
    "                    print(f\"\\\\old{{{old_segment}}}\", end='')\n",
    "                elif token_tag == 'insert':\n",
    "                    print(f\"\\\\new{{{new_segment}}}\", end='')\n",
    "            print()  # newline after each line\n",
    "        print()  # newline after token-level output\n",
    "        # print(f\"  REPLACE: '{old_lines[i1:i2]}'\\n -> '{new_lines[j1:j2]}'\")\n",
    "    elif tag == 'delete':\n",
    "        print(f\"  DELETE:  '{old_lines[i1:i2]}'\")\n",
    "    elif tag == 'insert':\n",
    "        print(f\"  INSERT:  '{new_lines[j1:j2]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Matcher for LaTeX markup changes\n",
    "\n",
    "For your specific requirement, we need a token-level comparison that:\n",
    "1. Treats LaTeX commands as single tokens (e.g., `\\textbf` stays together)\n",
    "2. Keeps braces with their content context\n",
    "3. Shows the OLD text without markup and NEW text with markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with your example:\n",
      "OLD: \\def\\myabstract{Companies in asset-intensive industries, such as aviation}\n",
      "NEW: \\def\\myabstract{Companies in \\textbf{asset-intensive industries}, such as aviation}\n",
      "\n",
      "Old tokens: ['\\\\def', '\\\\myabstract', '{', 'Companies', ' ', 'in', ' ', 'asset', '-', 'intensive', ' ', 'industries', ',', ' ', 'such', ' ', 'as', ' ', 'aviation', '}']\n",
      "New tokens: ['\\\\def', '\\\\myabstract', '{', 'Companies', ' ', 'in', ' ', '\\\\textbf', '{', 'asset', '-', 'intensive', ' ', 'industries', '}', ',', ' ', 'such', ' ', 'as', ' ', 'aviation', '}']\n",
      "\n",
      "Opcodes:\n",
      "equal    old[0:7] = ['\\\\def', '\\\\myabstract', '{', 'Companies', ' ', 'in', ' ']\n",
      "         new[0:7] = ['\\\\def', '\\\\myabstract', '{', 'Companies', ' ', 'in', ' ']\n",
      "\n",
      "insert   old[7:7] = []\n",
      "         new[7:9] = ['\\\\textbf', '{']\n",
      "\n",
      "equal    old[7:12] = ['asset', '-', 'intensive', ' ', 'industries']\n",
      "         new[9:14] = ['asset', '-', 'intensive', ' ', 'industries']\n",
      "\n",
      "insert   old[12:12] = []\n",
      "         new[14:15] = ['}']\n",
      "\n",
      "equal    old[12:20] = [',', ' ', 'such', ' ', 'as', ' ', 'aviation', '}']\n",
      "         new[15:23] = [',', ' ', 'such', ' ', 'as', ' ', 'aviation', '}']\n",
      "\n",
      "Result:\n",
      "\\def\\myabstract{Companies in \\new{\\textbf{}asset-intensive industries\\new{}}, such as aviation}\n"
     ]
    }
   ],
   "source": [
    "# Test with your specific example\n",
    "old_example = r\"\\def\\myabstract{Companies in asset-intensive industries, such as aviation}\"\n",
    "new_example = r\"\\def\\myabstract{Companies in \\textbf{asset-intensive industries}, such as aviation}\"\n",
    "\n",
    "print(\"Testing with your example:\")\n",
    "print(f\"OLD: {old_example}\")\n",
    "print(f\"NEW: {new_example}\")\n",
    "print()\n",
    "\n",
    "# Tokenize\n",
    "old_tokens = tokenize_latex(old_example)\n",
    "new_tokens = tokenize_latex(new_example)\n",
    "\n",
    "print(f\"Old tokens: {old_tokens}\")\n",
    "print(f\"New tokens: {new_tokens}\")\n",
    "print()\n",
    "\n",
    "# Compare\n",
    "matcher = difflib.SequenceMatcher(None, old_tokens, new_tokens)\n",
    "\n",
    "print(\"Opcodes:\")\n",
    "for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "    print(f\"{tag:8s} old[{i1}:{i2}] = {old_tokens[i1:i2]}\")\n",
    "    print(f\"         new[{j1}:{j2}] = {new_tokens[j1:j2]}\")\n",
    "    print()\n",
    "\n",
    "# Generate output\n",
    "output_tokens = []\n",
    "for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "    old_segment = old_tokens[i1:i2]\n",
    "    new_segment = new_tokens[j1:j2]\n",
    "    \n",
    "    if tag == 'equal':\n",
    "        output_tokens.extend(old_segment)\n",
    "    elif tag == 'replace':\n",
    "        output_tokens.extend(['\\\\old{'] + old_segment + ['}\\\\new{'] + new_segment + ['}'])\n",
    "    elif tag == 'delete':\n",
    "        output_tokens.extend(['\\\\old{'] + old_segment + ['}'])\n",
    "    elif tag == 'insert':\n",
    "        output_tokens.extend(['\\\\new{'] + new_segment + ['}'])\n",
    "\n",
    "result = ''.join(output_tokens)\n",
    "print(\"Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped tokens:\n",
      "Old: ['\\\\def', '\\\\myabstract', '{', 'Companies', ' ', 'in', ' ', 'asset', '-', 'intensive', ' ', 'industries', ',', ' ', 'such', ' ', 'as', ' ', 'aviation', '}']\n",
      "New: ['\\\\def', '\\\\myabstract', '{', 'Companies', ' ', 'in', ' ', '\\\\textbf{asset-intensive industries}', ',', ' ', 'such', ' ', 'as', ' ', 'aviation', '}']\n",
      "\n",
      "Opcodes with grouped tokens:\n",
      "equal    old[0:7] = ['\\\\def', '\\\\myabstract', '{', 'Companies', ' ', 'in', ' ']\n",
      "         new[0:7] = ['\\\\def', '\\\\myabstract', '{', 'Companies', ' ', 'in', ' ']\n",
      "\n",
      "replace  old[7:12] = ['asset', '-', 'intensive', ' ', 'industries']\n",
      "         new[7:8] = ['\\\\textbf{asset-intensive industries}']\n",
      "\n",
      "equal    old[12:20] = [',', ' ', 'such', ' ', 'as', ' ', 'aviation', '}']\n",
      "         new[8:16] = [',', ' ', 'such', ' ', 'as', ' ', 'aviation', '}']\n",
      "\n",
      "Result with selective grouping:\n",
      "\\def\\myabstract{Companies in \\old{asset-intensive industries}\\new{\\textbf{asset-intensive industries}}, such as aviation}\n"
     ]
    }
   ],
   "source": [
    "def group_latex_commands(tokens):\n",
    "    \"\"\"\n",
    "    Group ONLY formatting LaTeX commands with their arguments into single tokens.\n",
    "    E.g., ['\\\\textbf', '{', 'text', '}'] becomes ['\\\\textbf{text}']\n",
    "    \n",
    "    This helps the matcher see '\\textbf{asset-intensive industries}' as a unit\n",
    "    rather than separate tokens, which causes it to be recognized as a REPLACEMENT\n",
    "    of plain text with formatted text.\n",
    "    \n",
    "    Only groups these formatting commands:\n",
    "    - Text formatting: textbf, textit, texttt, textsc, emph, underline, etc.\n",
    "    - Font commands: bf, it, tt, sc, rm, sf\n",
    "    - Size commands: tiny, small, large, Large, LARGE, huge, Huge\n",
    "    \"\"\"\n",
    "    # Commands that should be grouped with their arguments\n",
    "    FORMATTING_COMMANDS = {\n",
    "        'textbf', 'textit', 'texttt', 'textsc', 'textrm', 'textsf',\n",
    "        'emph', 'underline', 'textsl', 'textmd', 'textup',\n",
    "        'bf', 'it', 'tt', 'sc', 'rm', 'sf', 'sl', 'md', 'up',\n",
    "        'tiny', 'scriptsize', 'footnotesize', 'small', 'normalsize',\n",
    "        'large', 'Large', 'LARGE', 'huge', 'Huge',\n",
    "        'textcolor', 'color', 'colorbox',\n",
    "    }\n",
    "    \n",
    "    result = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        \n",
    "        # Check if this is a formatting command followed by braces\n",
    "        if token.startswith('\\\\') and len(token) > 1:\n",
    "            cmd_name = token[1:]  # Remove the backslash\n",
    "            \n",
    "            # Only group if it's a formatting command\n",
    "            if cmd_name in FORMATTING_COMMANDS and i + 1 < len(tokens) and tokens[i + 1] == '{':\n",
    "                # Find matching closing brace\n",
    "                brace_count = 0\n",
    "                group = [token]\n",
    "                j = i + 1\n",
    "                \n",
    "                while j < len(tokens):\n",
    "                    group.append(tokens[j])\n",
    "                    if tokens[j] == '{':\n",
    "                        brace_count += 1\n",
    "                    elif tokens[j] == '}':\n",
    "                        brace_count -= 1\n",
    "                        if brace_count == 0:\n",
    "                            # Found complete command with arguments\n",
    "                            result.append(''.join(group))\n",
    "                            i = j + 1\n",
    "                            break\n",
    "                    j += 1\n",
    "                else:\n",
    "                    # No matching brace found, just add the command\n",
    "                    result.append(token)\n",
    "                    i += 1\n",
    "            else:\n",
    "                # Not a formatting command, or no braces - add as-is\n",
    "                result.append(token)\n",
    "                i += 1\n",
    "        else:\n",
    "            # Not a command, add as-is\n",
    "            result.append(token)\n",
    "            i += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Test the grouping function\n",
    "old_tokens_raw = tokenize_latex(old_example)\n",
    "new_tokens_raw = tokenize_latex(new_example)\n",
    "\n",
    "old_tokens_grouped = group_latex_commands(old_tokens_raw)\n",
    "new_tokens_grouped = group_latex_commands(new_tokens_raw)\n",
    "\n",
    "print(\"Grouped tokens:\")\n",
    "print(f\"Old: {old_tokens_grouped}\")\n",
    "print(f\"New: {new_tokens_grouped}\")\n",
    "print()\n",
    "\n",
    "# Now compare with grouped tokens\n",
    "matcher = difflib.SequenceMatcher(None, old_tokens_grouped, new_tokens_grouped)\n",
    "\n",
    "print(\"Opcodes with grouped tokens:\")\n",
    "for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "    print(f\"{tag:8s} old[{i1}:{i2}] = {old_tokens_grouped[i1:i2]}\")\n",
    "    print(f\"         new[{j1}:{j2}] = {new_tokens_grouped[j1:j2]}\")\n",
    "    print()\n",
    "\n",
    "# Generate output\n",
    "output_tokens = []\n",
    "for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "    old_segment = old_tokens_grouped[i1:i2]\n",
    "    new_segment = new_tokens_grouped[j1:j2]\n",
    "    \n",
    "    if tag == 'equal':\n",
    "        output_tokens.extend(old_segment)\n",
    "    elif tag == 'replace':\n",
    "        output_tokens.append('\\\\old{' + ''.join(old_segment) + '}\\\\new{' + ''.join(new_segment) + '}')\n",
    "    elif tag == 'delete':\n",
    "        output_tokens.append('\\\\old{' + ''.join(old_segment) + '}')\n",
    "    elif tag == 'insert':\n",
    "        output_tokens.append('\\\\new{' + ''.join(new_segment) + '}')\n",
    "\n",
    "result = ''.join(output_tokens)\n",
    "print(\"Result with selective grouping:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latex-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
